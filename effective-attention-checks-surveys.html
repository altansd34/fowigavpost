<!doctype html><html lang=en><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=MobileOptimized content="320"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=referrer content="no-referrer"><meta name=description content="We can measure the difficulty of an attention check by calculating its failure rate, the share of respondents who answered the question in a way that might indicate dishonesty, a lack of attention, or non-compliance with instructions. In our survey, failure rates varied widely; from less than one percent to well over 50%. The study"><meta name=robots content="index,follow,noarchive"><link href="https://fonts.googleapis.com/css?family=Open+Sans:400|Old+Standard+TT:400&display=swap" rel=stylesheet media=print type=text/css onload='this.media="all"'><title>The Most Effective Attention Checks in Surveys</title><link rel=canonical href=./effective-attention-checks-surveys.html><style>*{border:0;font:inherit;font-size:100%;vertical-align:baseline;margin:0;padding:0;color:#000;text-decoration-skip:ink}body{font-family:open sans,myriad pro,Myriad,sans-serif;font-size:17px;line-height:160%;color:#1d1313;max-width:700px;margin:auto}p{margin:20px 0}a img{border:none}img{margin:10px auto;max-width:100%;display:block}.left-justify{float:left}.right-justify{float:right}pre,code{font:12px Consolas,liberation mono,Menlo,Courier,monospace;background-color:#f7f7f7}code{font-size:12px;padding:4px}pre{margin-top:0;margin-bottom:16px;word-wrap:normal;padding:16px;overflow:auto;font-size:85%;line-height:1.45}pre>code{padding:0;margin:0;font-size:100%;word-break:normal;white-space:pre;background:0 0;border:0}pre code{display:inline;padding:0;margin:0;overflow:visible;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}pre code::before,pre code::after{content:normal}em,q,em,dfn{font-style:italic}.sans,html .gist .gist-file .gist-meta{font-family:open sans,myriad pro,Myriad,sans-serif}.mono,pre,code,tt,p code,li code{font-family:Menlo,Monaco,andale mono,lucida console,courier new,monospace}.heading,.serif,h1,h2,h3{font-family:old standard tt,serif}strong{font-weight:600}q:before{content:"\201C"}q:after{content:"\201D"}del,s{text-decoration:line-through}blockquote{font-family:old standard tt,serif;text-align:center;padding:50px}blockquote p{display:inline-block;font-style:italic}blockquote:before,blockquote:after{font-family:old standard tt,serif;content:'\201C';font-size:35px;color:#403c3b}blockquote:after{content:'\201D'}hr{width:40%;height:1px;background:#403c3b;margin:25px auto}h1{font-size:35px}h2{font-size:28px}h3{font-size:22px;margin-top:18px}h1 a,h2 a,h3 a{text-decoration:none}h1,h2{margin-top:28px}#sub-header,.date{color:#403c3b;font-size:13px}#sub-header{margin:0 4px}#nav h1 a{font-size:35px;color:#1d1313;line-height:120%}.posts_listing a,#nav a{text-decoration:none}li{margin-left:20px}ul li{margin-left:5px}ul li{list-style-type:none}ul li:before{content:"\00BB \0020"}#nav ul li:before,.posts_listing li:before{content:'';margin-right:0}#content{text-align:left;width:100%;font-size:15px;padding:60px 0 80px}#content h1,#content h2{margin-bottom:5px}#content h2{font-size:25px}#content .entry-content{margin-top:15px}#content .date{margin-left:3px}#content h1{font-size:30px}.highlight{margin:10px 0}.posts_listing{margin:0 0 50px}.posts_listing li{margin:0 0 25px 15px}.posts_listing li a:hover,#nav a:hover{text-decoration:underline}#nav{text-align:center;position:static;margin-top:60px}#nav ul{display:table;margin:8px auto 0}#nav li{list-style-type:none;display:table-cell;font-size:15px;padding:0 20px}#links{display:flex;justify-content:space-between;margin:50px 0 0}#links :nth-child(1){margin-right:.5em}#links :nth-child(2){margin-left:.5em}#not-found{text-align:center}#not-found a{font-family:old standard tt,serif;font-size:200px;text-decoration:none;display:inline-block;padding-top:225px}@media(max-width:750px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:28px}#nav li{font-size:13px;padding:0 15px}#content{margin-top:0;padding-top:50px;font-size:14px}#content h1{font-size:25px}#content h2{font-size:22px}.posts_listing li div{font-size:12px}}@media(max-width:400px){body{padding-left:20px;padding-right:20px}#nav h1 a{font-size:22px}#nav li{font-size:12px;padding:0 10px}#content{margin-top:0;padding-top:20px;font-size:12px}#content h1{font-size:20px}#content h2{font-size:18px}.posts_listing li div{font-size:12px}}@media(prefers-color-scheme:dark){*,#nav h1 a{color:#fdfdfd}body{background:#121212}pre,code{background-color:#262626}#sub-header,.date{color:#bababa}hr{background:#ebebeb}}</style></head><body><section id=nav><h1><a href=./index.html>PixBlog</a></h1><ul><li><a href=./index.xml>Rss</a></li><li><a href=./sitemap.xml>Sitemap</a></li></ul></section><section id=content><h1>The Most Effective Attention Checks in Surveys</h1><div id=sub-header>August 2024 · 8 minute read</div><div class=entry-content><h2 id=most-difficult-attention-checks><strong>Which attention checks are the most difficult?<br></strong></h2><p><span>We can measure the difficulty of an attention check by calculating its failure rate, the share of respondents who answered the question in a way that might indicate dishonesty, a lack of attention, or non-compliance with instructions. In our survey, failure rates varied widely; from less than one percent to well over 50%. </span><b>The study produced a few key findings related to attention check difficulty:&nbsp;&nbsp;</b><b></b></p><li><b>Attention checks that rely on response consistency tend to have low failure rates.&nbsp;</b></li><li aria-level=1><b>With one exception, more direct attention checks were less difficult than checks that attempted to hide the purpose of the question. </b><span>For example, questions that asked respondents to solve a simple math problem or identify common word associations had failure rates between 3% and 6%, while questions that tried to trick respondents by, e.g., asking whether respondents use a fake product, had failure rates between 11% and 49%.&nbsp;</span></li><li aria-level=1><b>Attention checks with confusing instructions may have unacceptably high failure rates.</b><span> In our standalone directed choice question, respondents were first instructed to select only one item from a list. Then they were told to ignore the previous instructions and select two specific items. More than 50% of respondents failed this attention check, many of whom did select one of the items they were instructed to pick but not the other.</span></li><p><span>The wide range of failure rates across the different screeners suggests that researchers need to carefully pretest attention checks to make sure that attentive respondents understand the instructions and can complete the screener in the way the researcher expects.</span></p><h3 id=attention-check-failure-rates><strong><b>Table 1: Attention check types, descriptions and failure rates</b></strong></h3><table id=tablepress-464 class="tablepress tablepress-id-464"><tr class=row-1><th class=column-1>Screener Name</th><th class=column-2>Description</th><th class=column-3>Failure Rate</th></tr><tbody class=row-hover readability=14><tr class=row-2 readability=5><td class=column-1>Standalone Directed Choice</td><td class=column-2>Respondents are directed to select a specific response regardless of their true preferences (ignoring the question itself). Respondents who do not select the correct response fail the screener</td><td class=column-3>> 50%</td></tr><tr class=row-3 readability=3><td class=column-1>Grid Trap Question</td><td class=column-2>One item is added to a grid that can only be reasonably answered one way. Respondents who don’t select the appropriate response fail the screener</td><td class=column-3>28%</td></tr><tr class=row-4 readability=2><td class=column-1>Straight Line</td><td class=column-2>Respondents who select the same scale point for over 90% of a grid fail the screener.</td><td class=column-3>12%</td></tr><tr class=row-5 readability=3><td class=column-1>Grid Directed Choice</td><td class=column-2>An item is added to a grid question that instructs respondents to select a specific response. Respondents who don’t select this response fail the screener.</td><td class=column-3>10%</td></tr><tr class=row-6 readability=6><td class=column-1>Word Association</td><td class=column-2>Respondents are given a word (e.g., “book”) and asked to select the word most closely associated with the word (e.g., “pen”). Respondents who do not select the correct response fail the screener.</td><td class=column-3>6%</td></tr><tr class=row-7 readability=3><td class=column-1>Multiple Choice Math</td><td class=column-2>Respondents are asked to solve a random arithmetic problem and select the correct answer from a list. Respondents who select the wrong answer fail the screener.</td><td class=column-3>5%</td></tr><tr class=row-8 readability=3><td class=column-1>Open End Math</td><td class=column-2>Respondents are asked to solve a random arithmetic problem and type their answer in a text box. Respondents who type the wrong answer fail the screener.</td><td class=column-3>3%</td></tr><tr class=row-9 readability=3><td class=column-1>Consistency Check</td><td class=column-2>Respondents are asked the same question twice. Those who provide inconsistent answers fail the screener.</td><td class=column-3>1%</td></tr></tbody></table><h2 id=most-effective-attention-checks><strong>Which attention checks are the most effective?</strong></h2><p><span>Attention checks cannot be evaluated based on their difficulty alone; after all, there is no “correct” failure rate a priori. Attention checks that are too difficult risk incorrectly flagging good respondents as inattentive, and attention checks that are too easy can lead to poor quality data </span><a href=#><span>that can impact a survey’s estimates</span></a><span>.</span></p><p>To evaluate the effectiveness of different attention checks, we replicated the approach described by <a href=#>Berinsky et al (2019)</a>. <span>This approach involves using an experiment with a well-known, often-replicated result, and comparing the results of the experiment among those who pass versus fail each attention check. In this survey, we included </span><a href=#><span>Tversky and Kahneman’s (1981)</span></a><span> disease framing experiment:</span></p><p><i><span>Imagine that your country is preparing for the outbreak of an unusual disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume that the exact scientific estimates of the consequences of the programs are as follows:&nbsp;</span></i></p><p><i><span>(Lives Saved Frame)</span></i></p><p><i><span>“If Program A is adopted, 200 people will be saved. If Program B is adopted, there is 1/3 probability that 600 people will be saved, and 2/3 probability that no people will be saved.”</span></i></p><p><i><span>(Mortality Frame)&nbsp;</span></i></p><p><i><span>“If Program A is adopted, 400 people will die. If Program B is adopted there is 1/3 probability that nobody will die, and 2/3 probability that 600 people will die.”&nbsp;</span></i></p><p><span>Even though the two sets of plans are exactly the same, respondents who see the “Lives Saved” frame tend to be much more likely to select Program A, while respondents who see the “Mortality” frame tend to be more likely to select Program B. The difference between the proportion who choose program A in each frame constitutes the “treatment effect.”</span></p><p><span>Because inattentive respondents are not likely to actually read the framing text, we do not expect them to be influenced by the framing. Instead, we expect these respondents to choose between the options quasi-randomly. As a result, we can use the treatment effect as a metric to judge the effectiveness of each screener question. An effective screener should show a very low treatment effect among respondents who failed the screener (because they are not paying attention, and therefore are not being fully treated) and a much higher treatment effect among respondents who passed the screener. We calculated the average treatment effect among respondents who passed and failed each type of screener in our survey and present the results in Figure 2.&nbsp;</span></p><h3 id=average-treatment-effect><strong><b>Figure 2: Average treatment effect by screener</b></strong></h3><p><img decoding=async src=https://cdn.statically.io/img/assets.morningconsult.com/wp-uploads/2023/10/09010347/kt_effect_by_screener.png alt="average treatment effect by screener" width=1800 height=1800 sizes="(max-width: 1800px) 100vw, 1800px" style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><span id=response-options><span>Note: The dotted line indicates the “expected” treatment effect based on Berinsky, Margolis, and Sances (2014)</span></span></p><p>The top-performing attention check type was the directed choice item embedded within a grid. <span>There is a very low treatment effect among respondents who failed the screener, and quite a high treatment effect among those who passed the screener.</span></p><p><span>Interestingly, the three best-performing screeners were all different types of attention checks — one grid trap question, one directed choice grid item, and one word association task.&nbsp;</span></p><p><span>Some screeners appear to do an excellent job of separating “medium” attention respondents from “high” attention respondents. For example, respondents who passed “Grid Trap Question 2” were among the most attentive respondents, but some respondents who failed this screener seemed to be paying attention to the survey (see the relatively high treatment effect among failures).&nbsp;</span><span>Even attention screeners that perform well may not be able to effectively separate attentive respondents from inattentive ones on their own. Figure 2 includes a dotted line to indicate the “expected” treatment effect based on other studies that have used this same framing experiment. With the exception of the grid trap questions and standalone directed choice item, even respondents who passed a single attention screener have a slightly lower-than-expected treatment effect. As extant research suggests, effectively identifying attentive respondents often </span><a href=# rel=noopener><span>requires a set of multiple attention checks</span></a><span>.</span></p><h2 id=constructing-an-attentiveness-scale><strong>Constructing an attentiveness scale</strong></h2><p><span>Now that we have measured the difficulty and effectiveness of each screener, we can use that information to create a scale of overall attention. The resulting score allows us to either remove respondents who fall below a given threshold or </span><a href=# rel=noopener><span>stratify our survey estimates based on attentiveness</span></a><span>.</span></p><p><span>To construct our attentiveness scale, we start by selecting the five attention checks that do the best job of separating inattentive respondents from attentive ones. Using these five items, we could simply count the number of passed attention checks and use this count as our attention score. This additive index approach works reasonably well but gives equal weight to every screener.&nbsp;</span></p><p><span>As we saw above, some screeners provide more information about overall attentiveness than others. We can incorporate this additional information in a variety of ways. Again following Berinsky et al. (2019), we use a two-parameter item response theory (IRT) model to construct latent attentiveness for each respondent based on the five top-performing attention checks. Figure 3 shows how, using either the additive index or the IRT model, respondents at the higher end of the attention scale appear to be paying more attention to the framing experiment.&nbsp;</span></p><p><span>Although both scales can identify attentive respondents, the IRT-derived scale does so much more efficiently. Using the simple count scale, the treatment effect only reaches the expected level among respondents who passed all five screeners. When using the IRT-derived scale, on the other hand, the top 3/5ths of the attentiveness scale reaches the expected treatment effect. Looking at the bottom of the attentiveness scale shows that even removing the bottom quintile of the attention score yields the same treatment effect as removing anyone who passed fewer than three screeners. This evidence suggests that some screeners provide much more information about attentiveness than others, and relying on a simple count ignores that crucial fact. Researchers should think carefully about adding the right attention checks and how to most effectively use the information they provide.</span></p><h3 id=treatment-effects-attentiveness-scale><strong><b>Figure 3: Treatment effects are much higher among respondents who score higher on the attention scale</b></strong></h3><p><img decoding=async src=https://cdn.statically.io/img/assets.morningconsult.com/wp-uploads/2023/10/09010556/kt_effect_pass_count-300x300.png alt="KT effect pass count" width=370 height=370 data-wp-editing=1 sizes="(max-width: 370px) 100vw, 370px" style=margin:auto;display:block;text-align:center><img decoding=async src=https://cdn.statically.io/img/assets.morningconsult.com/wp-uploads/2023/10/09010723/kt_effect_top5-300x300.png alt="KT Effect Top 5" width=367 height=367 sizes="(max-width: 367px) 100vw, 367px" style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p><span id=response-options><span>Note: The dotted line indicates the “expected” treatment effect based on Berinsky, Margolis, and Sances (2014)</span></span></p><h2 id=importance-of-effective-attention-checks><strong>Attention checks help improve data quality but require care in implementation</strong></h2><p><span>When it comes to building a high-quality survey program, attention checks are essential. But it isn’t enough to simply add a random attention check you find online — as we’ve shown, some attention checks are </span><a href=# rel=noopener><span>more difficult to pass</span></a><span>, even for respondents who are paying attention, and some attention checks are much better at distinguishing between attentive and inattentive respondents. Using the wrong set of attention checks risks removing good respondents from your survey and letting bad respondents through. This can seriously impact your survey results; as we’ve shown above, inattentive respondents can lead to attenuated treatment effects and introduce bias to your estimates.</span></p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmiln6e7qrrGnKanq6WhwW%2BvzqZma2hiaHxyfI5qaWidlpuypMDIr5xmmaSpsq%2FAyKilZpuYmrCsv4ysrKuula7AcA%3D%3D</p></div><div id=links><a href=./1590774-the-truth-dane-cooks-brother-darryl-mccauley-now.html>&#171;&nbsp;The truth about Dane Cook's brother, Darryl McCauley, and where he is now</a>
<a href=./1458193-swagboyqs-biography-age-height-real-girlfriend-net-worth.html>SwagBoyQs biography: age, height, real name, girlfriend, net worth&nbsp;&#187;</a></div></section><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://iklan.listspress.com/tracking_server_3.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>